{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EmOpti Workshop - AutoML\n",
    "\n",
    "Kernel `Python 3 (Data Science)` works well with this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "session = sagemaker.Session()\n",
    "s3bucket = session.default_bucket()\n",
    "s3prefix = \"emopti\"\n",
    "\n",
    "role = get_execution_role()\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the dataset to Amazon S3\n",
    "Copy the file to Amazon Simple Storage Service (Amazon S3) in a .csv format for Amazon SageMaker training to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'train.csv'\n",
    "test_filename = 'test.csv'\n",
    "\n",
    "train_data_s3path = session.upload_data(bucket=s3bucket, path=f'data/{train_filename}', key_prefix=f'{s3prefix}/automl/data')\n",
    "print(\"Train data uploaded to: \" + train_data_s3path)\n",
    "\n",
    "test_data_s3path = session.upload_data(bucket=s3bucket, path=f'data/{test_filename}', key_prefix=f'{s3prefix}/automl/data')\n",
    "print(\"Test data uploaded to: \" + test_data_s3path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the SageMaker Autopilot Job<a name=\"Settingup\"></a>\n",
    "\n",
    "After uploading the dataset to Amazon S3, you can invoke Autopilot to find the best ML pipeline to train a model on this dataset. \n",
    "\n",
    "The required inputs for invoking a Autopilot job are:\n",
    "* Amazon S3 location for input dataset and for all output artifacts\n",
    "* Name of the column of the dataset you want to predict (`calc_disp` in this case) \n",
    "* An IAM role\n",
    "\n",
    "Currently Autopilot supports only tabular datasets in CSV format. \n",
    "\n",
    "Either all files should have a header row, or the first file of the dataset, when sorted in alphabetical/lexical order, is expected to have a header row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ml_job_config = {\n",
    "    \"CompletionCriteria\": {\n",
    "        \"MaxCandidates\": 5\n",
    "    }\n",
    "}\n",
    "\n",
    "input_data_config = [\n",
    "    {\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": f's3://{s3bucket}/{s3prefix}/automl/data/train.csv'\n",
    "            }\n",
    "        },\n",
    "        \"TargetAttributeName\": \"calc_disp\",\n",
    "    }\n",
    "]\n",
    "\n",
    "output_data_config = {\n",
    "    \"S3OutputPath\": f's3://{s3bucket}/{s3prefix}/automl/results/training'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the type of problem you want to solve with your dataset (`Regression, MulticlassClassification, BinaryClassification`). In case you are not sure, SageMaker Autopilot will infer the problem type based on statistics of the target column (the column you want to predict). \n",
    "\n",
    "You have the option to limit the running time of a SageMaker Autopilot job by providing either the maximum number of pipeline evaluations or candidates (one pipeline evaluation is called a `Candidate` because it generates a candidate model) or providing the total time allocated for the overall Autopilot job. Under default settings, this job takes about four hours to run. This varies between runs because of the nature of the exploratory process Autopilot uses to find optimal training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching the SageMaker Autopilot Job<a name=\"Launching\"></a>\n",
    "\n",
    "You can now launch the Autopilot job by calling the `create_auto_ml_job` API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "auto_ml_job_name = f'automl-job-{strftime(\"%Y%m%d-%H%M\", gmtime())}'\n",
    "print(\"AutoMLJobName: \" + auto_ml_job_name)\n",
    "\n",
    "sm.create_auto_ml_job(\n",
    "    AutoMLJobName=auto_ml_job_name,\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    AutoMLJobConfig=auto_ml_job_config,\n",
    "    # ProblemType='BinaryClassification'|'MulticlassClassification'|'Regression',\n",
    "    ProblemType='BinaryClassification',\n",
    "    # AutoMLJobObjective = {'MetricName': 'Accuracy'|'MSE'|'F1'|'F1macro'|'AUC'}\n",
    "    AutoMLJobObjective={'MetricName': 'F1'},    \n",
    "    RoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking SageMaker AutoPilot job progress<a name=\"Tracking\"></a>\n",
    "SageMaker AutoPilot job consists of the following high-level steps : \n",
    "* Analyzing Data, where the dataset is analyzed and Autopilot comes up with a list of ML pipelines that should be tried out on the dataset. The dataset is also split into train and validation sets.\n",
    "* Feature Engineering, where Autopilot performs feature transformation on individual features of the dataset as well as at an aggregate level.\n",
    "* Model Tuning, where the top performing pipeline is selected along with the optimal hyperparameters for the training algorithm (the last stage of the pipeline). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"JobStatus - Secondary Status\")\n",
    "print(\"------------------------------\")\n",
    "\n",
    "\n",
    "describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "print(describe_response[\"AutoMLJobStatus\"] + \" - \" + describe_response[\"AutoMLJobSecondaryStatus\"])\n",
    "job_run_status = describe_response[\"AutoMLJobStatus\"]\n",
    "\n",
    "while job_run_status not in (\"Failed\", \"Completed\", \"Stopped\"):\n",
    "    describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "    job_run_status = describe_response[\"AutoMLJobStatus\"]\n",
    "\n",
    "    print(\n",
    "        describe_response[\"AutoMLJobStatus\"] + \" - \" + describe_response[\"AutoMLJobSecondaryStatus\"]\n",
    "    )\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "describe_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Now use the describe_auto_ml_job API to look up the best candidate selected by the SageMaker Autopilot job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_candidate = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)[\"BestCandidate\"]\n",
    "best_candidate_name = best_candidate[\"CandidateName\"]\n",
    "print(best_candidate)\n",
    "print(\"\\n\")\n",
    "print(\"CandidateName: \" + best_candidate_name)\n",
    "print(\n",
    "    \"FinalAutoMLJobObjectiveMetricName: \"\n",
    "    + best_candidate[\"FinalAutoMLJobObjectiveMetric\"][\"MetricName\"]\n",
    ")\n",
    "print(\n",
    "    \"FinalAutoMLJobObjectiveMetricValue: \"\n",
    "    + str(best_candidate[\"FinalAutoMLJobObjectiveMetric\"][\"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform batch inference using the best candidate\n",
    "\n",
    "Now that you have successfully completed the SageMaker Autopilot job on the dataset, create a model from any of the candidates by using [Inference Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'automl-model-{strftime(\"%Y%m%d-%H%M\", gmtime())}'\n",
    "\n",
    "model = sm.create_model(\n",
    "    Containers=best_candidate[\"InferenceContainers\"], ModelName=model_name, ExecutionRoleArn=role\n",
    ")\n",
    "\n",
    "print(\"Model ARN corresponding to the best candidate is : {}\".format(model[\"ModelArn\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use batch inference by using Amazon SageMaker batch transform. The same model can also be deployed to perform online inference using Amazon SageMaker hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_job_name = f'automl-transform-{strftime(\"%Y%m%d-%H%M\", gmtime())}'\n",
    "\n",
    "transform_input = {\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\", \n",
    "            \"S3Uri\": f's3://{s3bucket}/{s3prefix}/automl/data/test.csv'\n",
    "        }},\n",
    "    \"ContentType\": \"text/csv\",\n",
    "    \"CompressionType\": \"None\",\n",
    "    \"SplitType\": \"Line\",\n",
    "}\n",
    "\n",
    "transform_output = {\n",
    "    \"S3OutputPath\": f\"s3://{s3bucket}/{s3prefix}/automl/results/inference\",\n",
    "}\n",
    "\n",
    "transform_resources = {\n",
    "    \"InstanceType\": \"ml.m5.4xlarge\", \n",
    "    \"InstanceCount\": 1\n",
    "}\n",
    "\n",
    "sm.create_transform_job(\n",
    "    TransformJobName=transform_job_name,\n",
    "    ModelName=model_name,\n",
    "    TransformInput=transform_input,\n",
    "    TransformOutput=transform_output,\n",
    "    TransformResources=transform_resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch the transform job for completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"JobStatus\")\n",
    "print(\"----------\")\n",
    "\n",
    "\n",
    "describe_response = sm.describe_transform_job(TransformJobName=transform_job_name)\n",
    "job_run_status = describe_response[\"TransformJobStatus\"]\n",
    "print(job_run_status)\n",
    "\n",
    "while job_run_status not in (\"Failed\", \"Completed\", \"Stopped\"):\n",
    "    describe_response = sm.describe_transform_job(TransformJobName=transform_job_name)\n",
    "    job_run_status = describe_response[\"TransformJobStatus\"]\n",
    "    print(job_run_status)\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Predictions (the results of our transform job):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s3_output_key = f\"{s3prefix}/automl/results/inference/test.csv.out\"\n",
    "local_inference_results_path = \"automl-inference_results.csv\"\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "inference_results_bucket = s3.Bucket(s3bucket)\n",
    "inference_results_bucket.download_file(s3_output_key, local_inference_results_path)\n",
    "\n",
    "df_preds = pd.read_csv(local_inference_results_path, sep=\";\")\n",
    "pd.set_option(\"display.max_rows\", 20)  # Keep the output on one page\n",
    "df_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View other candidates explored by SageMaker Autopilot\n",
    "You can view all the candidates (pipeline evaluations with different hyperparameter combinations) that were explored by SageMaker Autopilot and sort them by their final performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = sm.list_candidates_for_auto_ml_job(\n",
    "    AutoMLJobName=auto_ml_job_name, SortBy=\"FinalObjectiveMetricValue\"\n",
    ")[\"Candidates\"]\n",
    "index = 1\n",
    "for candidate in candidates:\n",
    "    print(\n",
    "        str(index)\n",
    "        + \"  \"\n",
    "        + candidate[\"CandidateName\"]\n",
    "        + \"  \"\n",
    "        + str(candidate[\"FinalAutoMLJobObjectiveMetric\"][\"Value\"])\n",
    "    )\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Labels for our Test Data and show only the ADMIT rows so we can see the count of ADMITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df_test = pd.read_csv('data/test_labels.csv')\n",
    "df_test[df_test['DISCHARGE'] == 'ADMIT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df_test, df_preds[1:])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "#labels = [f'True Neg\\n{cm[0][0]}', f'False Pos\\n{cm[0][1]}', f'False Neg\\n{cm[1][0]}', f'True Pos\\n{cm[1][1]}']\n",
    "#labels = np.asarray(labels).reshape(2,2)\n",
    "ax = sns.heatmap(cm, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_xticklabels(['ADMIT', 'DISCHARGE'])\n",
    "ax.set_yticklabels(['ADMIT', 'DISCHARGE'])\n",
    "ax.set(ylabel = \"True Label\", xlabel = \"Predicted Label\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Definition Notebook\n",
    "    \n",
    "Sagemaker AutoPilot also auto-generates a Candidate Definitions notebook. This notebook can be used to interactively step through the various steps taken by the Sagemaker Autopilot to arrive at the best candidate. This notebook can also be used to override various runtime parameters like parallelism, hardware used, algorithms explored, feature extraction scripts and more.\n",
    "    \n",
    "The notebook can be downloaded from the following Amazon S3 location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3notebook = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)[\"AutoMLJobArtifacts\"][\n",
    "    \"CandidateDefinitionNotebookLocation\"\n",
    "]\n",
    "s3notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $s3notebook ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration Notebook\n",
    "Sagemaker Autopilot also auto-generates a Data Exploration notebook, which can be downloaded from the following Amazon S3 location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3notebook = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)[\"AutoMLJobArtifacts\"][\n",
    "    \"DataExplorationNotebookLocation\"\n",
    "]\n",
    "s3notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $s3notebook ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
