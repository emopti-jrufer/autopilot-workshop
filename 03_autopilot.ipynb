{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EmOpti Workshop - AutoPilot Batch Transform\n",
    "\n",
    "This notebook runs the same test data through a trained model that was created by AutoPilot\n",
    "\n",
    "Kernel `Python 3 (Data Science)` works well with this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "session = sagemaker.Session()\n",
    "s3bucket = session.default_bucket()\n",
    "s3bucket = 'am-tmp2'\n",
    "s3prefix = \"emopti\"\n",
    "\n",
    "role = get_execution_role()\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Artifact\n",
    "You must enter the S3 URL of a Model created by AutoPilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact = 's3://am-tmp2/emopti2/emopti2-exp1/tuning/emopti2-ex-dpp2-xgb/emopti2-exp1KTbVpBNcXGZQGbIuIKlA-102-17f0ab7a/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Model \n",
    "Creating a Model makes it easy to create a *Batch Transform* job using the Model specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "model_name = f'autopilot-model-{strftime(\"%Y%m%d-%H%M\", gmtime())}'\n",
    "container = get_image_uri(boto3.Session().region_name, \"xgboost\", \"1.3-1\")\n",
    "\n",
    "model = sm.create_model(\n",
    "    ModelName = model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': model_artifact,\n",
    "        'Environment': {}\n",
    "    },\n",
    "    ExecutionRoleArn = role\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our Model is created, let's create our *Batch Transform* job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_job_name = f'autopilot-transform-{strftime(\"%Y%m%d-%H%M\", gmtime())}'\n",
    "\n",
    "transform_input = {\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\", \n",
    "            \"S3Uri\": f's3://{s3bucket}/{s3prefix}/automl/data/test.csv'\n",
    "        }},\n",
    "    \"ContentType\": \"text/csv\",\n",
    "    \"CompressionType\": \"None\",\n",
    "    \"SplitType\": \"Line\",\n",
    "}\n",
    "\n",
    "transform_output = {\n",
    "    \"S3OutputPath\": f\"s3://{s3bucket}/{s3prefix}/autopilot/results/inference\",\n",
    "}\n",
    "\n",
    "transform_resources = {\n",
    "    \"InstanceType\": \"ml.m5.4xlarge\", \n",
    "    \"InstanceCount\": 1\n",
    "}\n",
    "\n",
    "sm.create_transform_job(\n",
    "    TransformJobName=transform_job_name,\n",
    "    ModelName=model_name,\n",
    "    TransformInput=transform_input,\n",
    "    TransformOutput=transform_output,\n",
    "    TransformResources=transform_resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the *Batch Transform* job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"JobStatus\")\n",
    "print(\"----------\")\n",
    "\n",
    "describe_response = sm.describe_transform_job(TransformJobName=transform_job_name)\n",
    "job_run_status = describe_response[\"TransformJobStatus\"]\n",
    "print(job_run_status)\n",
    "\n",
    "while job_run_status not in (\"Failed\", \"Completed\", \"Stopped\"):\n",
    "    describe_response = sm.describe_transform_job(TransformJobName=transform_job_name)\n",
    "    job_run_status = describe_response[\"TransformJobStatus\"]\n",
    "    print(job_run_status)\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the results file from our *Batch Transform* job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s3_output_key = f\"{s3prefix}/autopilot/results/inference/test.csv.out\"\n",
    "local_inference_results_path = \"autopilot-inference_results.csv\"\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "inference_results_bucket = s3.Bucket(s3bucket)\n",
    "inference_results_bucket.download_file(s3_output_key, local_inference_results_path)\n",
    "\n",
    "df_preds = pd.read_csv(local_inference_results_path, sep=\";\")\n",
    "pd.set_option(\"display.max_rows\", 20)  # Keep the output on one page\n",
    "df_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Labels for our Test Data and show only the ADMIT rows so we can see the count of ADMITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df_test = pd.read_csv('data/test_labels.csv')\n",
    "df_test[df_test['DISCHARGE'] == 'ADMIT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df_test, data[1:])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "#labels = [f'True Neg\\n{cm[0][0]}', f'False Pos\\n{cm[0][1]}', f'False Neg\\n{cm[1][0]}', f'True Pos\\n{cm[1][1]}']\n",
    "#labels = np.asarray(labels).reshape(2,2)\n",
    "ax = sns.heatmap(cm, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_xticklabels(['ADMIT', 'DISCHARGE'])\n",
    "ax.set_yticklabels(['ADMIT', 'DISCHARGE'])\n",
    "ax.set(ylabel = \"True Label\", xlabel = \"Predicted Label\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
